{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1f693f-efae-42ac-ae4c-759085d236f2",
   "metadata": {},
   "source": [
    "# Uncertainty demo: how to build a neural network that knows when it doesn't know\n",
    "* Based on 2D toy datasets\n",
    "* Classification problems\n",
    "* Contrasts approximate Bayesian methods (MC dropout), empirically strong baselines on high-dimensional data (deep ensembles) with distance-aware methods (recent developments)\n",
    "* This notebook constitutes the practical demo for [_Towards a scalable deployment of AI models via Uncertainty Quantification_, _Open Data Science Conference, London 2022_](https://odsc.com/speakers/towards-a-scalable-deployment-of-ai-models-via-uncertainty-quantification/). See the accompanying slide deck for background and references to literature.\n",
    "* A very similar tutorial, but for SNGP using Gaussian Processes instead of simpler density models can be found at: https://www.tensorflow.org/tutorials/understanding/sngp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558310e2-7c2e-4a65-bb27-bf29c1628b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.datasets\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import official.nlp.modeling.layers as nlp_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6137bc11-70ac-4c36-a1ce-0c0a6080e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b3ada-8fb9-46b1-a6d2-05013394c863",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc105b9-27bb-4895-8a4a-a3ac9362c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_RANGE = (-3, 3)\n",
    "X2_RANGE = (-3, 3)\n",
    "N_GRID = 100\n",
    "\n",
    "\n",
    "def generate_train_data(n_samples=1000, dataset=\"moons\"):\n",
    "    if dataset == \"moons\":\n",
    "        return sklearn.datasets.make_moons(n_samples=n_samples, noise=0.1)\n",
    "    elif dataset == \"circles\":\n",
    "        return sklearn.datasets.make_circles(n_samples=n_samples, noise=0.1)\n",
    "\n",
    "\n",
    "def generate_test_data():\n",
    "    x1 = np.linspace(X1_RANGE[0], X1_RANGE[1], N_GRID)\n",
    "    x2 = np.linspace(X2_RANGE[0], X2_RANGE[1], N_GRID)\n",
    "    x1v, x2v = np.meshgrid(x1, x2)\n",
    "    return np.c_[x1v.flatten(), x2v.flatten()]\n",
    "\n",
    "\n",
    "def plot_training_data(X_train, y_train, ax):\n",
    "    colors = sns.color_palette()\n",
    "    for k in classes:\n",
    "        ax.scatter(X_train[y_train==k, 0], X_train[y_train==k, 1], color=colors[k], label='class %d' %k)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b1baa-ca3a-4fd4-9e7f-0984ffc5425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_train_data(dataset=\"moons\")\n",
    "X_test = generate_test_data()\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "fig = plt.figure()\n",
    "plot_training_data(X_train, y_train, plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1515a7f-0d9c-461e-9b51-4771fd361a0e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c5f67b-9a8c-4dea-94a9-1c2db1db318f",
   "metadata": {},
   "source": [
    "We start by defining some functions that we will reuse throughout for different models:\n",
    "* All models will be deep neural networks with the same overall architecture (number of layers and units per layer) that will just differ in terms of their hidden layer specification\n",
    "* All models will be trained with the same hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28518e16-c146-4e0b-a96a-d9a9dd4b402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layer_class: tf.keras.layers.Layer, layer_config={}, n_features=2, n_classes=2, n_layers=8) -> tf.keras.Model:\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(n_features,))\n",
    "    # Projection of input to n_hidden dimensions such that layer can apply skip connections\n",
    "    x = tf.keras.layers.Dense(layer_config.get(\"n_hidden\", 128), trainable=False)(inputs)\n",
    "    \n",
    "    for _ in range(n_layers):\n",
    "        x = layer_class(**layer_config)(x)\n",
    "    logits = tf.keras.layers.Dense(n_classes)(x)\n",
    "    score = tf.keras.activations.softmax(logits, axis=-1)[:, 0]  # 0th class score\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs={\"features\": x, \"logits\": logits, \"score\": score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473c9c6-f9d0-4594-aa7b-17f7e2f228f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: tf.keras.Model, X_train: np.ndarray, y_train: np.ndarray):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss={\"logits\": tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)}\n",
    "    )\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c7524-60a1-4a47-8095-de62b3c2a8cc",
   "metadata": {},
   "source": [
    "### Deterministic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff58eb-11c0-4294-ae1b-276d298842d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Dense layer with dropout for building a standard multi-layer perceptron\"\"\"\n",
    "    \n",
    "    def __init__(self, n_hidden=128, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(n_hidden, activation=\"relu\")\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb846e0-f2cf-4773-a4b6-61e948d4cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(MLPLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348bde02-a01a-4410-9dbe-a4db0cf933c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd72a77-05ae-4e23-9ab5-45c81b4d296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72a383-62d3-40ca-8b1d-782bba9f5349",
   "metadata": {},
   "source": [
    "## Quantifying uncertainty\n",
    "Some measures that are useful for aleatoric uncertainty, but not necessarily for epistemic uncertainty as demonstrated below. As long as they don't change the rank order, it does not really matter which one we take for aleatoric uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686a0a9-22fe-4f12-b2d1-62fb36e27ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_entropy(p):\n",
    "    eps = np.finfo(float).eps\n",
    "    return -(p * np.log2(p + eps) + (1 - p) * np.log2((1 - p) + eps))\n",
    "\n",
    "\n",
    "def binary_variance(p):\n",
    "    return p * (1 - p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2fda00-7277-4743-8894-094ddc8d6eff",
   "metadata": {},
   "source": [
    "## Visualizing predictions and uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907dbfb-216d-4b18-8d9e-b4cac0bcd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scores(scores, ax):\n",
    "        \n",
    "    # Score surface\n",
    "    score_surface = ax.imshow(\n",
    "        scores.reshape([N_GRID, N_GRID]),\n",
    "        interpolation=\"bicubic\",\n",
    "        origin=\"lower\",\n",
    "        extent=X1_RANGE + X2_RANGE,\n",
    "    )\n",
    "    fig.colorbar(score_surface, ax=ax)\n",
    "    \n",
    "    # Training data\n",
    "    plot_training_data(X_train, y_train, ax)\n",
    "\n",
    "\n",
    "def visualize_predictions_and_uncertainty(y_score_test: np.ndarray, uncertainties: Dict[str, np.ndarray]):\n",
    "    \n",
    "    n_uncertainties = len(uncertainties.keys())\n",
    "    n_cols = 1 + n_uncertainties\n",
    "    fig, axs = plt.subplots(1, n_cols, figsize=(14 * n_cols, 10))\n",
    "    \n",
    "    visualize_scores(y_score_test, axs[0])\n",
    "    axs[0].set_title(\"Model scores p(y|x)\")\n",
    "    \n",
    "    for i, (name, uncertainty) in enumerate(uncertainties.items()): \n",
    "        visualize_scores(uncertainty, axs[i + 1])\n",
    "        axs[i + 1].set_title(f\"Uncertainty ({name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ecf8e-a997-4fdb-af3b-c78f46cbc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5aa2c-6003-49d3-a338-eba82720c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions_and_uncertainty(outputs_test[\"score\"], {\"entropy\": binary_entropy(outputs_test[\"score\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b62845-8dd1-4b3a-80b3-1a79769ed5e6",
   "metadata": {},
   "source": [
    "## MC dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942348a-18df-4a8d-8d8f-7de85bf1109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predictive_mean(model, X_test, n_samples=10):\n",
    "    \n",
    "    def mc_sample():\n",
    "        return model(X_test, training=True)[\"score\"].numpy()\n",
    "    \n",
    "    return np.mean([mc_sample() for _ in range(n_samples)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254191fa-6570-4409-ad7f-953303e533ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mc_dropout_predictive_mean(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a66a3c-ed83-4822-97b1-58bf3ea7d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions_and_uncertainty(score, {\"entropy\": binary_entropy(score)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8166983-c1e2-4531-bf96-e28264cf47f2",
   "metadata": {},
   "source": [
    "## Deep ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b0a80-2098-4061-9533-9ae161151b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(X_train, y_train, n_members=5):\n",
    "    trained_models = []\n",
    "    for _ in tqdm(range(n_members)):\n",
    "        model = build_model(MLPLayer, layer_config={})\n",
    "        train(model, X_train, y_train)\n",
    "        trained_models.append(model)\n",
    "    return trained_models\n",
    "\n",
    "\n",
    "def ensemble_predictive_mean(models, X_test):\n",
    "    return np.mean([model.predict(X_test)[\"score\"] for model in models], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eff4aa-340e-421b-b120-9105e76ea0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = train_ensemble(X_train, y_train, n_members=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d67ed-9d45-4ee0-be89-7d5705d82080",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = ensemble_predictive_mean(trained_models, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5058d34-1ba6-4be7-ac0d-c5c6dd441f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions_and_uncertainty(score, {\"entropy\": binary_entropy(score)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84cc9f-dc72-46ff-a759-9bf4885622bb",
   "metadata": {},
   "source": [
    "## Distance-aware uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99943083-d2f6-4718-95ea-862ab5604524",
   "metadata": {},
   "source": [
    "### (Spectrally normalized) ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be72cf6-4daf-47f3-a79e-d4c5e8315618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetSNLayer(MLPLayer):\n",
    "    \"\"\"Same as a hidden layer for a MLP, but spectrally normalized and with skip connections\"\"\"\n",
    "    \n",
    "    def __init__(self, norm_multiplier=0.9, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.spectrally_normalised_dense = nlp_layers.SpectralNormalization(\n",
    "            self.dense, norm_multiplier=norm_multiplier)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.spectrally_normalised_dense(inputs)\n",
    "        x = self.dropout(x)\n",
    "        return inputs + x\n",
    "    \n",
    "\n",
    "class ResNetLayer(MLPLayer):\n",
    "    \"\"\"Same as a hidden layer for a MLP, but with a skip connection\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + super().call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a847d2-b8e9-4c93-85f1-0252e0d686db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(ResNetSNLayer, layer_config={}, n_layers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092bb9e3-0624-4f67-8752-6bd6837020bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf55c5-1e44-4071-bae9-0c6b13607309",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ce9f8-6f48-460a-8005-337c542e0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions_and_uncertainty(outputs_test[\"score\"], {\"entropy\": binary_entropy(outputs_test[\"score\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e88cb1-b125-4f4f-8be0-0f66e85a8aaf",
   "metadata": {},
   "source": [
    "### Some simple density / OOD models\n",
    "Following the thoughts of [Mukhoti et al. 2022: Deep Deterministic Uncertainty: A Simple Baseline](https://arxiv.org/pdf/2102.11582v3.pdf#cite.van2021improving). For [Liu et al. 2020: Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness, NeurIPS](https://proceedings.neurips.cc/paper/2020/hash/543e83748234f7cbab21aa0ade66565f-Abstract.html), see the SNGP tutorial [here](https://www.tensorflow.org/tutorials/understanding/sngp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e44db-4d83-4852-abc0-6c486af5a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31d0c6-1c13-4e9a-8623-e5f0b738bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    \"\"\"Gaussian mixture model\n",
    "    \n",
    "    with one component per ground truth class.\n",
    "    \n",
    "    Using the ground truth information from training data\n",
    "    avoids the costly unsupervised component assignment via EM\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        classes = np.unique(y_train)\n",
    "        n_classes = len(classes)\n",
    "        n_total, n_features = X_train.shape\n",
    "        \n",
    "        self.pi = np.zeros(n_classes)\n",
    "        self.components = []\n",
    "\n",
    "        for i, k in enumerate(classes):\n",
    "            n_k = sum(y_train == k)\n",
    "            self.pi[i] = n_k / n_total\n",
    "            X_k = X_train[y_train == k]\n",
    "            self.components.append(\n",
    "                multivariate_normal(\n",
    "                    mean=np.mean(X_k, axis=0), \n",
    "                    cov=np.cov(X_k.T),\n",
    "                    allow_singular=True,  # depends on numerical definition\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def score_samples(self, X_test):\n",
    "        \"\"\"Compute log-density for each sample\"\"\"\n",
    "        # We compute everything in log space to avoid numerical instabilities\n",
    "        log_p_per_component = [np.log(pi_k) + component.logpdf(X_test) for pi_k, component in zip(self.pi, self.components)]\n",
    "        return np.logaddexp(*log_p_per_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad8280-24b3-4d5a-a72d-5c95c939ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bedaf7-adc5-4e08-93ac-c330a3887cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_and_normalize_score(score, minimum):\n",
    "    \"\"\"Normalize to make different uncertainty quantification approaches comparable\"\"\"\n",
    "    score = score.copy()\n",
    "    # Pin minimum to that of training data as we \n",
    "    # are not interested in differences outside of\n",
    "    # training distribution\n",
    "    score[score<=minimum] = minimum\n",
    "    # Scale to [0, 1] interval \n",
    "    score -= minimum\n",
    "    score /= score.max()\n",
    "    # Reverse scale such that OoD samples achieve high scores\n",
    "    return 1 - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d9f26-508d-48f6-81a4-9991123dc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train, feat_test = outputs_train[\"features\"], outputs_test[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14902fda-76b7-4d2a-b6ef-60f3d9de3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = GMM().fit(feat_train, y_train)\n",
    "#scorer = GaussianMixture(n_components=2).fit(feat_train)\n",
    "\n",
    "score_test = scorer.score_samples(feat_test)\n",
    "score_train = scorer.score_samples(feat_train)\n",
    "visualize_predictions_and_uncertainty(\n",
    "    outputs_test[\"score\"],\n",
    "    {scorer.__class__.__name__: threshold_and_normalize_score(score_test, minimum=score_train.min())}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41cb04-aee3-4ef0-b61b-659dd7d7fe36",
   "metadata": {},
   "source": [
    "### Ablations\n",
    "Hyperparameters that are interesting to vary: \n",
    "* Network capacity (number of layers and units)\n",
    "* Layer architecture (with/without skip connections, spectral normalisation)\n",
    "* Type of density model\n",
    "* Influence of dimensionality reduction (e.g. via PCA on hidden feature space. Ignoring dimensions with little variance on training data allows significant reduction of dimensionality but may lead to feature collaps for test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee3b98-7c72-4183-8f63-088a480b3e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19ec6d-94d6-47d6-8364-78aa1cf1c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(layers, scorers, n_layers):\n",
    "    \n",
    "    for layer_class, layer_config in layers:\n",
    "        model = build_model(layer_class, layer_config=layer_config, n_layers=n_layers)\n",
    "        train(model, X_train, y_train)\n",
    "\n",
    "        outputs_train = model.predict(X_train)\n",
    "        outputs_test = model.predict(X_test)\n",
    "    \n",
    "        # Distance-aware uncertainties\n",
    "        uncertainties = {}\n",
    "        for scorer in scorers:\n",
    "            scorer = scorer.fit(outputs_train[\"features\"])\n",
    "            score_train = scorer.score_samples(outputs_train[\"features\"])\n",
    "            score_test = scorer.score_samples(outputs_test[\"features\"])\n",
    "            uncertainties[scorer.__class__.__name__] = threshold_and_normalize_score(score_test, minimum=score_train.min())\n",
    "        \n",
    "        visualize_predictions_and_uncertainty(outputs_test[\"score\"], uncertainties)\n",
    "        plt.suptitle(f\"Discriminative feature space [layer]: {layer_class.__name__}\", size=\"xx-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b6125a-2813-4f38-aedd-0addcd31aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers=10\n",
    "\n",
    "train_and_evaluate(\n",
    "    layers=[\n",
    "        (ResNetSNLayer, {\"norm_multiplier\": 0.9}),\n",
    "        (MLPLayer, {}),\n",
    "        (ResNetLayer, {}),\n",
    "    ],\n",
    "    scorers=[\n",
    "        GaussianMixture(n_components=2),\n",
    "        OneClassSVM(),\n",
    "        IsolationForest(),\n",
    "    ],\n",
    "    n_layers=n_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f53afc8-ebce-466d-8567-19a577cc2b5b",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e935f-836e-44a0-8b27-9ad8aed635b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb6290-1cf0-4120-adcf-0c58fb24f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(X_train, X_test):\n",
    "    pca = decomposition.PCA(n_components=0.99, whiten=True)\n",
    "    return pca.fit_transform(X_train), pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f95407-ee3f-41f4-ae1c-6d6306509b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train, feat_test = reduce_dimensionality(outputs_train[\"features\"], outputs_test[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bede8b05-18e5-4c9b-91ca-495826a37cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
